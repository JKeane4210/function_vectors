{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking Function Vectors\n",
    "\n",
    "**Jonathan Keane | November 16, 2025**\n",
    "\n",
    "To understand more of the nuances behind what function vectors are and how they are computed, I wanted to do some extra logging and documenting of how this works in practice before proceeding on to more experiments.\n",
    "\n",
    "If you have not read the paper associated with this work, a **function vector** is a vector that can be inserted into the residual stream of a transformer in place of a certain token's internal embedding at any layer such that it can \"apply an operation\" in arbitrary contexts. So for example, if given the sentence `left :` with no context, you could replace the `:`'s internal embedding with a function vector to make the model produce an output like `right` (antonym function vector) or `izquierda` (translation function vector). This is done through extracting information from transformer internal embeddings when a transformer is in the process of doing in-context learning (ICL) for the same task, which will be described further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "EDIT_LAYER = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For a series of different tasks, there are just pairs of input text and their corresponding output text. In the example below, we are working with antonym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input\": [\n",
      "    \"limitless\",\n",
      "    \"wake\",\n",
      "    \"elevate\",\n",
      "    \"push\"\n",
      "  ],\n",
      "  \"output\": [\n",
      "    \"limited\",\n",
      "    \"sleep\",\n",
      "    \"depress\",\n",
      "    \"pull\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "print(json.dumps(dataset['train'][:4], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Task-Conditioned Mean Activations\n",
    "\n",
    "To create a function vector, you need to have activations of tokens from places where in-context learning (ICL) was occurring across ***all*** attention heads in the transformer (although only some will be used in practice). To do this, for a series of trials, a set of random samples from a task's corresponding dataset will be used to get activations resulting from all attention heads in the transformer. This will be used to generate activations from all attention heads in the transformers for all tokens in all trial prompts, resulting in a matrix of activations of shape $(n\\_trials, n\\_layers, n\\_heads, n\\_tokens, d\\_head)$, which will then be averaged along the trial dimension to get the average in-context learning activations that are generated from samples in this task.\n",
    "\n",
    "***Comment:*** This is a really interesting paradigm because it seems to almost create a \"training process\" while working with a trained LLM to start (sort of a way of doing model distillation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer, n_icl_examples=10, N_TRIALS=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### In-Context Examples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Trial 1: Input | Trial 1: Output | Trial 2: Input | Trial 2: Output |\n",
       "| --- | --- | --- | --- |\n",
       "| enlarge | shrink | solitary | social |\n",
       "| same | different | succeed | fail |\n",
       "| expected | unexpected | worldwide | local |\n",
       "| decrypt | encrypt | indirect | direct |\n",
       "| robust | fragile | employ | unemploy |\n",
       "| solution | problem | everyday | occasional |\n",
       "| amusing | boring | fragile | durable |\n",
       "| childhood | adulthood | intrinsic | extrinsic |\n",
       "| ordinary | extraordinary | foster | hinder |\n",
       "| **Query**: acidic | **Target**: alkaline | **Query**: undesirable | **Target**: desirable |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Activations Shape: torch.Size([28, 16, 97, 256])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.prompt_utils import get_dummy_token_labels\n",
    "from src.utils.extract_utils import gather_attn_activations\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer, n_icl_examples = 10, N_TRIALS = 100, shuffle_labels=False, prefixes=None, separators=None, filter_set=None):\n",
    "    def split_activations_by_head(activations, model_config):\n",
    "        new_shape = activations.size()[:-1] + (model_config['n_heads'], model_config['resid_dim']//model_config['n_heads']) # split by head: + (n_attn_heads, hidden_size/n_attn_heads)\n",
    "        activations = activations.view(*new_shape)  # (batch_size, n_tokens, n_heads, head_hidden_dim)\n",
    "        return activations\n",
    "\n",
    "    n_test_examples = 1\n",
    "    if prefixes is not None and separators is not None:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, prefixes=prefixes, separators=separators, model_config=model_config)\n",
    "    else:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, model_config=model_config)\n",
    "    activation_storage = torch.zeros(N_TRIALS, model_config['n_layers'], model_config['n_heads'], len(dummy_labels), model_config['resid_dim']//model_config['n_heads'])\n",
    "\n",
    "    if filter_set is None:\n",
    "        filter_set = np.arange(len(dataset['valid']))\n",
    "\n",
    "    # If the model already prepends a bos token by default, we don't want to add one\n",
    "    prepend_bos =  False if model_config['prepend_bos'] else True\n",
    "\n",
    "    word_pair_info = []\n",
    "    for n in range(N_TRIALS):\n",
    "        word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "        word_pairs_test = dataset['valid'][np.random.choice(filter_set,n_test_examples, replace=False)]\n",
    "        if prefixes is not None and separators is not None:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                    shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "        else:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n",
    "        word_pair_info.append({'word_pairs': word_pairs, 'word_pairs_test': word_pairs_test})\n",
    "        activations_td,idx_map,idx_avg = gather_attn_activations(prompt_data=prompt_data, \n",
    "                                                            layers = model_config['attn_hook_names'], \n",
    "                                                            dummy_labels=dummy_labels, \n",
    "                                                            model=model, \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            model_config=model_config)\n",
    "\n",
    "        stack_initial = torch.vstack([split_activations_by_head(activations_td[layer].input, model_config) for layer in model_config['attn_hook_names']]).permute(0,2,1,3)\n",
    "        stack_filtered = stack_initial[:,:,list(idx_map.keys())]\n",
    "        for (i,j) in idx_avg.values():\n",
    "            stack_filtered[:,:,idx_map[i]] = stack_initial[:,:,i:j+1].mean(axis=2) # Average activations of multi-token words across all its tokens\n",
    "        \n",
    "        activation_storage[n] = stack_filtered\n",
    "\n",
    "    mean_activations = activation_storage.mean(dim=0) # averaging over trials (not tokens => 97 represents n_tokens)\n",
    "    return mean_activations, word_pair_info\n",
    "\n",
    "mean_activations, word_pair_info = get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer)\n",
    "\n",
    "def tabulate(data, headers):\n",
    "    table = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "    for row in data[1:]:\n",
    "        table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "    return table\n",
    "\n",
    "n_samples_display = 2\n",
    "table_data_headers = []\n",
    "table_data = []\n",
    "for sample_idx in range(n_samples_display):\n",
    "    sample_data = word_pair_info[sample_idx]\n",
    "    sample_table_data = [\n",
    "        [\n",
    "            sample_data[\"word_pairs\"][\"input\"][i], \n",
    "            sample_data[\"word_pairs\"][\"output\"][i]\n",
    "        ] \n",
    "        for i in range(len(sample_data[\"word_pairs\"][\"input\"]))\n",
    "    ]\n",
    "    sample_table_data.append([\n",
    "        f'**Query**: {sample_data[\"word_pairs_test\"][\"input\"][0]}', \n",
    "        f'**Target**: {sample_data[\"word_pairs_test\"][\"output\"][0]}',\n",
    "    ])\n",
    "    if len(table_data) == 0:\n",
    "        table_data = sample_table_data\n",
    "    else:\n",
    "        for i in range(len(sample_table_data)):\n",
    "            table_data[i].extend(sample_table_data[i])\n",
    "    table_data_headers.extend([f\"Trial {sample_idx+1}: Input\", f\"Trial {sample_idx+1}: Output\"])\n",
    "\n",
    "display(Markdown(\"### In-Context Examples\"))\n",
    "display(Markdown(tabulate(table_data, table_data_headers)))\n",
    "print(\"Mean Activations Shape:\", mean_activations.shape) # (n_layers, n_heads, n_tokens, head_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Function Vector (FV)\n",
    "\n",
    "While the previous step looked to create the activations for every attention head in every layer for every token, part of the work done in this paper was the process of building function vectors using activations from only the attention heads that are most likely to contribute this more general capability of in-context learning.\n",
    "\n",
    "To filter down to the most important heads that should be used for building the function vector, you first just want to look at the attention head output activations associated to the final token of an in-context learning prompt (such as the `:` in the example discussed in the intro). Then, you choose the $n$ attention heads that have the highest impact on in-context learning related tasks.\n",
    "\n",
    "**How do you evaluate high impact on in-context learning?**\n",
    "\n",
    "The authors propose a metric called average indirect effect (AIE). This metric works by taking attention head activations for a prompt with random relationships being defined as a baseline for performance. Then, by replacing single attention heads with the average attention head activation observed for the in-context learning samples, an estimate for how important a head is can be made.\n",
    "\n",
    "***Comment:*** The authors hard code these weights in the implementations of the demo because it seems that similar heads are activate across different tasks. This relates similarly to some interpretability work that Anthropic has done that looks at induction heads as heads that are better in repeating similar phrases during in-context learning.\n",
    "\n",
    "- *The authors did do some further analysis of this in their appendix and found that while there is some overlap between the heads they identified as top candidates for making function vectors and heads that show signs of being induction heads, they are not completely overlapping (which could mean that there could be future work in distinguishing these different kinds of heads well equipped for in-context learning).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Vector Shape: torch.Size([1, 4096])\n",
      "Top Heads: [(15, 5, 0.0587), (9, 14, 0.0584), (12, 10, 0.0526), (8, 1, 0.0445), (11, 0, 0.0445), (13, 13, 0.019), (8, 0, 0.0184), (14, 9, 0.016), (9, 2, 0.0127), (24, 6, 0.0113)]\n"
     ]
    }
   ],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)\n",
    "print(\"Function Vector Shape:\", FV.shape) # (n_layers, n_heads, head_hidden_dim)\n",
    "print(\"Top Heads:\", top_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: health\\n\\nQ: fascism\\nA: ignore\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: software\\n\\nQ: notice\\nA: democracy\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: increase\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.73675), (' reduce', 0.07769), (' increase', 0.03435), (' decline', 0.01574), (' decreased', 0.01037)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: health\\n\\nQ: fascism\\nA: ignore\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: software\\n\\nQ: notice\\nA: democracy\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Few-Shot-Shuffled Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.26971), (' reduce', 0.02771), (' increase', 0.02549), (' decline', 0.01609), (' growth', 0.00921)] \n",
      "\n",
      "Shuffled Prompt+FV Top K Vocab Probs:\n",
      " [(' decrease', 0.75362), (' reduce', 0.03991), (' decline', 0.02322), (' increase', 0.00935), (' reduction', 0.00837)]\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Zero-Shot Top K Vocab Probs:\n",
      " [(' increase', 0.14925), (' yes', 0.02272), (' I', 0.02189), (' the', 0.0212), (' 1', 0.01418)] \n",
      "\n",
      "Zero-Shot+FV Vocab Top K Vocab Probs:\n",
      " [(' decrease', 0.25627), (' increase', 0.1799), (' reduce', 0.03497), (' improve', 0.00987), ('\\n', 0.00582)]\n"
     ]
    }
   ],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"increase\" means'\n",
      "GPT-J: 'The word \"increase\" means \"to make larger, to enlarge, to expand'\n",
      "GPT-J+FV: 'The word \"increase\" means \"decrease\" in the Bible.\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jonny Additions\n",
    "\n",
    "### Zero-Shot Application to New Q/A Pair\n",
    "\n",
    "This is cool and all that it remembers to say decrease for this example, but is this an actual \"antonym\" function vector (this is how I had originally interpreted the paper, but the examples don't show this off). I test with a couple pairs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"happy\" means'\n",
      "GPT-J: 'The word \"happy\" means different things to different people. For some, it'\n",
      "GPT-J+FV: 'The word \"happy\" means \"sad\" to me.\\n\\nI' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'happy', 'output': 'sad'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"yin\" means'\n",
      "GPT-J: 'The word \"yin\" means \"darkness\" or \"black\" in Chinese'\n",
      "GPT-J+FV: 'The word \"yin\" means \"yang.\" The word \"yin\" means' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'yin', 'output': 'yang'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What does the embedding of the original token look like if it does not have as much in-context learning effecting it?\n",
    "\n",
    "    - If you increase/decrease the number of samples, is there a convergence to a single vector that represents opposites? (I feel like the embedding should start pretty spread out if you ablate certain samples that are part of the context and then converge as you get to a more complete set)\n",
    "\n",
    "2. Is it the embedding of the colon that is changing or is it the tokens around it that are changing in terms of their position?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
