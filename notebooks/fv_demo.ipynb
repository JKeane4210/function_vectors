{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc2e417aa524d30a97cb5a36ba3a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b115fb4aed3942f1b8b20ee88bf928b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "EDIT_LAYER = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: democracy\\n\\nQ: fascism\\nA: health\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: ignore\\n\\nQ: notice\\nA: software\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: increase\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Q: hardware\n",
      "A: software\n",
      "\n",
      "Q: fascism\n",
      "A: democracy\n",
      "\n",
      "Q: incompatible\n",
      "A: compatible\n",
      "\n",
      "Q: illness\n",
      "A: health\n",
      "\n",
      "Q: notice\n",
      "A: ignore\n",
      "\n",
      "Q: increase\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.73675), (' reduce', 0.07769), (' increase', 0.03435), (' decline', 0.01574), (' decreased', 0.01037)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Q: hardware\n",
      "A: democracy\n",
      "\n",
      "Q: fascism\n",
      "A: health\n",
      "\n",
      "Q: incompatible\n",
      "A: compatible\n",
      "\n",
      "Q: illness\n",
      "A: ignore\n",
      "\n",
      "Q: notice\n",
      "A: software\n",
      "\n",
      "Q: increase\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: ignore\\n\\nQ: fascism\\nA: health\\n\\nQ: incompatible\\nA: software\\n\\nQ: illness\\nA: compatible\\n\\nQ: notice\\nA: democracy\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Few-Shot-Shuffled Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.10689), (' increase', 0.01887), (' notice', 0.01787), (' reduce', 0.01295), (' democracy', 0.01078)] \n",
      "\n",
      "Shuffled Prompt+FV Top K Vocab Probs:\n",
      " [(' decrease', 0.70746), (' reduce', 0.04102), (' decline', 0.02279), (' increase', 0.00984), (' reduction', 0.00724)]\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Zero-Shot Top K Vocab Probs:\n",
      " [(' increase', 0.14925), (' yes', 0.02272), (' I', 0.02189), (' the', 0.0212), (' 1', 0.01418)] \n",
      "\n",
      "Zero-Shot+FV Vocab Top K Vocab Probs:\n",
      " [(' decrease', 0.27255), (' increase', 0.17896), (' reduce', 0.03427), (' improve', 0.00906), ('\\n', 0.00579)]\n"
     ]
    }
   ],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"increase\" means'\n",
      "GPT-J: 'The word \"increase\" means \"to make larger, to enlarge, to expand'\n",
      "GPT-J+FV: 'The word \"increase\" means \"decrease\" in the Bible.\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jonny Additions\n",
    "\n",
    "### Zero-Shot Application to New Q/A Pair\n",
    "\n",
    "This is cool and all that it remembers to say decrease for this example, but is this an actual \"antonym\" function vector (this is how I had originally interpreted the paper, but the examples don't show this off). I test with a couple pairs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"happy\" means'\n",
      "GPT-J: 'The word \"happy\" means different things to different people. For some, it'\n",
      "GPT-J+FV: 'The word \"happy\" means \"sad\" in the dictionary.\\n\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'happy', 'output': 'sad'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"yin\" means'\n",
      "GPT-J: 'The word \"yin\" means \"darkness\" or \"black\" in Chinese'\n",
      "GPT-J+FV: 'The word \"yin\" means \"yang.\" The word \"yin\" means' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'yin', 'output': 'yang'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What does the embedding of the original token look like if it does not have as much in-context learning effecting it?\n",
    "\n",
    "    - If you increase/decrease the number of samples, is there a convergence to a single vector that represents opposites? (I feel like the embedding should start pretty spread out if you ablate certain samples that are part of the context and then converge as you get to a more complete set)\n",
    "\n",
    "2. Is it the embedding of the colon that is changing or is it the tokens around it that are changing in terms of their position?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
