{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking Function Vectors\n",
    "\n",
    "**Jonathan Keane | November 16, 2025**\n",
    "\n",
    "To understand more of the nuances behind what function vectors are and how they are computed, I wanted to do some extra logging and documenting of how this works in practice before proceeding on to more experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "EDIT_LAYER = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For a series of different tasks, there are just pairs of input text and their corresponding output text. In the example below, we are working with antonym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input\": [\n",
      "    \"limitless\",\n",
      "    \"wake\",\n",
      "    \"elevate\",\n",
      "    \"push\"\n",
      "  ],\n",
      "  \"output\": [\n",
      "    \"limited\",\n",
      "    \"sleep\",\n",
      "    \"depress\",\n",
      "    \"pull\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "print(json.dumps(dataset['train'][:4], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Task-Conditioned Mean Activations\n",
    "\n",
    "To create a function vector, you need to have activations of tokens from places where in-context learning (ICL) was occurring across ***all*** attention heads in the transformer (although only some will be used in practice). To do this, for a series of trials, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer, n_icl_examples=10, N_TRIALS=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "torch.Size([10, 28, 16, 97, 256])\n",
      "torch.Size([28, 16, 97, 256])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### In-Context Examples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Trial 1: Input | Trial 1: Output | Trial 2: Input | Trial 2: Output |\n",
       "| --- | --- | --- | --- |\n",
       "| unusual | usual | relational | isolated |\n",
       "| soil | sky | worried | relaxed |\n",
       "| sturdy | fragile | artificial | natural |\n",
       "| mainland | island | unauthorized | authorized |\n",
       "| able | unable | wee | large |\n",
       "| daylight | nighttime | civil | uncivilized |\n",
       "| glad | sad | capable | incapable |\n",
       "| resent | cherish | federal | state |\n",
       "| pleased | displeased | integration | differentiation |\n",
       "| **Query**: transmitter | **Target**: receiver | **Query**: regain | **Target**: lose |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.utils.prompt_utils import get_dummy_token_labels\n",
    "from src.utils.extract_utils import gather_attn_activations\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer, n_icl_examples = 10, N_TRIALS = 10, shuffle_labels=False, prefixes=None, separators=None, filter_set=None):\n",
    "    def split_activations_by_head(activations, model_config):\n",
    "        new_shape = activations.size()[:-1] + (model_config['n_heads'], model_config['resid_dim']//model_config['n_heads']) # split by head: + (n_attn_heads, hidden_size/n_attn_heads)\n",
    "        activations = activations.view(*new_shape)  # (batch_size, n_tokens, n_heads, head_hidden_dim)\n",
    "        return activations\n",
    "\n",
    "    n_test_examples = 1\n",
    "    if prefixes is not None and separators is not None:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, prefixes=prefixes, separators=separators, model_config=model_config)\n",
    "    else:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, model_config=model_config)\n",
    "    print(len(dummy_labels))\n",
    "    activation_storage = torch.zeros(N_TRIALS, model_config['n_layers'], model_config['n_heads'], len(dummy_labels), model_config['resid_dim']//model_config['n_heads'])\n",
    "\n",
    "    if filter_set is None:\n",
    "        filter_set = np.arange(len(dataset['valid']))\n",
    "\n",
    "    # If the model already prepends a bos token by default, we don't want to add one\n",
    "    prepend_bos =  False if model_config['prepend_bos'] else True\n",
    "\n",
    "    word_pair_info = []\n",
    "    for n in range(N_TRIALS):\n",
    "        word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "        word_pairs_test = dataset['valid'][np.random.choice(filter_set,n_test_examples, replace=False)]\n",
    "        if prefixes is not None and separators is not None:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                    shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "        else:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n",
    "        word_pair_info.append({'word_pairs': word_pairs, 'word_pairs_test': word_pairs_test})\n",
    "        # print(prompt_data)\n",
    "        activations_td,idx_map,idx_avg = gather_attn_activations(prompt_data=prompt_data, \n",
    "                                                            layers = model_config['attn_hook_names'], \n",
    "                                                            dummy_labels=dummy_labels, \n",
    "                                                            model=model, \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            model_config=model_config)\n",
    "        # print(activations_td)\n",
    "        # print(idx_map)\n",
    "        # print(idx_avg)\n",
    "        # print(activations_td[layer].input.shape for layer in model_config['attn_hook_names'])\n",
    "        stack_initial = torch.vstack([split_activations_by_head(activations_td[layer].input, model_config) for layer in model_config['attn_hook_names']]).permute(0,2,1,3)\n",
    "        stack_filtered = stack_initial[:,:,list(idx_map.keys())]\n",
    "        for (i,j) in idx_avg.values():\n",
    "            stack_filtered[:,:,idx_map[i]] = stack_initial[:,:,i:j+1].mean(axis=2) # Average activations of multi-token words across all its tokens\n",
    "        \n",
    "        activation_storage[n] = stack_filtered\n",
    "\n",
    "    print(activation_storage.shape)\n",
    "    mean_activations = activation_storage.mean(dim=0) # averaging over trials (not tokens => 97 represents n_tokens)\n",
    "    print(mean_activations.shape)\n",
    "    return mean_activations, word_pair_info\n",
    "\n",
    "mean_activations, word_pair_info = get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer)\n",
    "\n",
    "def tabulate(data, headers):\n",
    "    table = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "    for row in data[1:]:\n",
    "        table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "    return table\n",
    "\n",
    "n_samples_display = 2\n",
    "table_data_headers = []\n",
    "table_data = []\n",
    "for sample_idx in range(n_samples_display):\n",
    "    sample_data = word_pair_info[sample_idx]\n",
    "    sample_table_data = [\n",
    "        [\n",
    "            sample_data[\"word_pairs\"][\"input\"][i], \n",
    "            sample_data[\"word_pairs\"][\"output\"][i]\n",
    "        ] \n",
    "        for i in range(len(sample_data[\"word_pairs\"][\"input\"]))\n",
    "    ]\n",
    "    sample_table_data.append([\n",
    "        f'**Query**: {sample_data[\"word_pairs_test\"][\"input\"][0]}', \n",
    "        f'**Target**: {sample_data[\"word_pairs_test\"][\"output\"][0]}',\n",
    "    ])\n",
    "    if len(table_data) == 0:\n",
    "        table_data = sample_table_data\n",
    "    else:\n",
    "        for i in range(len(sample_table_data)):\n",
    "            table_data[i].extend(sample_table_data[i])\n",
    "    table_data_headers.extend([f\"Trial {sample_idx+1}: Input\", f\"Trial {sample_idx+1}: Output\"])\n",
    "    \n",
    "display(Markdown(\"### In-Context Examples\"))\n",
    "display(Markdown(tabulate(table_data, table_data_headers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " '<|endoftext|>Q: hardware\\nA: health\\n\\nQ: fascism\\nA: ignore\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: software\\n\\nQ: notice\\nA: democracy\\n\\nQ: increase\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: increase\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Q: hardware\n",
      "A: software\n",
      "\n",
      "Q: fascism\n",
      "A: democracy\n",
      "\n",
      "Q: incompatible\n",
      "A: compatible\n",
      "\n",
      "Q: illness\n",
      "A: health\n",
      "\n",
      "Q: notice\n",
      "A: ignore\n",
      "\n",
      "Q: increase\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: software\\n\\nQ: fascism\\nA: democracy\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: health\\n\\nQ: notice\\nA: ignore\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.73675), (' reduce', 0.07769), (' increase', 0.03435), (' decline', 0.01574), (' decreased', 0.01037)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Q: hardware\n",
      "A: health\n",
      "\n",
      "Q: fascism\n",
      "A: ignore\n",
      "\n",
      "Q: incompatible\n",
      "A: compatible\n",
      "\n",
      "Q: illness\n",
      "A: software\n",
      "\n",
      "Q: notice\n",
      "A: democracy\n",
      "\n",
      "Q: increase\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: hardware\\nA: health\\n\\nQ: fascism\\nA: ignore\\n\\nQ: incompatible\\nA: compatible\\n\\nQ: illness\\nA: software\\n\\nQ: notice\\nA: democracy\\n\\nQ: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Few-Shot-Shuffled Prompt Top K Vocab Probs:\n",
      " [(' decrease', 0.26971), (' reduce', 0.02771), (' increase', 0.02549), (' decline', 0.01609), (' growth', 0.00921)] \n",
      "\n",
      "Shuffled Prompt+FV Top K Vocab Probs:\n",
      " [(' decrease', 0.75362), (' reduce', 0.03991), (' decline', 0.02322), (' increase', 0.00935), (' reduction', 0.00837)]\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: increase\\nA:' \n",
      "\n",
      "Input Query: 'increase', Target: 'decrease'\n",
      "\n",
      "Zero-Shot Top K Vocab Probs:\n",
      " [(' increase', 0.14925), (' yes', 0.02272), (' I', 0.02189), (' the', 0.0212), (' 1', 0.01418)] \n",
      "\n",
      "Zero-Shot+FV Vocab Top K Vocab Probs:\n",
      " [(' decrease', 0.25627), (' increase', 0.1799), (' reduce', 0.03497), (' improve', 0.00987), ('\\n', 0.00582)]\n"
     ]
    }
   ],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"increase\" means'\n",
      "GPT-J: 'The word \"increase\" means \"to make larger, to enlarge, to expand'\n",
      "GPT-J+FV: 'The word \"increase\" means \"decrease\" in the Bible.\\n' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jonny Additions\n",
    "\n",
    "### Zero-Shot Application to New Q/A Pair\n",
    "\n",
    "This is cool and all that it remembers to say decrease for this example, but is this an actual \"antonym\" function vector (this is how I had originally interpreted the paper, but the examples don't show this off). I test with a couple pairs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"happy\" means'\n",
      "GPT-J: 'The word \"happy\" means different things to different people. For some, it'\n",
      "GPT-J+FV: 'The word \"happy\" means \"sad\" to me.\\n\\nI' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'happy', 'output': 'sad'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  'The word \"yin\" means'\n",
      "GPT-J: 'The word \"yin\" means \"darkness\" or \"black\" in Chinese'\n",
      "GPT-J+FV: 'The word \"yin\" means \"yang.\" The word \"yin\" means' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_pair = {'input': 'yin', 'output': 'yang'}\n",
    "sentence = f\"The word \\\"{new_test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What does the embedding of the original token look like if it does not have as much in-context learning effecting it?\n",
    "\n",
    "    - If you increase/decrease the number of samples, is there a convergence to a single vector that represents opposites? (I feel like the embedding should start pretty spread out if you ablate certain samples that are part of the context and then converge as you get to a more complete set)\n",
    "\n",
    "2. Is it the embedding of the colon that is changing or is it the tokens around it that are changing in terms of their position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50400])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "clean_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50400, 4096)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-27): 28 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_vector_intervention Inputs:\n",
      "\tsentence='str:len=151',\n",
      "\target_outputs=['decrease'],\n",
      "\tEDIT_LAYER=9,\n",
      "\tFV=torch.Size([1, 4096]),\n",
      "\tmodel=model,\n",
      "\tmodel_config=model_config,\n",
      "\ttokenizer=tokenizer\n",
      "Outputs: clean_logits=torch.Size([1, 50400]), interv_logits=torch.Size([1, 50400])\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(f\"function_vector_intervention Inputs:\\n\"\n",
    "      f\"\\tsentence='str:len={len(shuffled_sentence)}',\\n\"\n",
    "      f\"\\target_outputs={[test_pair['output']]},\\n\"\n",
    "      f\"\\tEDIT_LAYER={EDIT_LAYER},\\n\"\n",
    "      f\"\\tFV={FV.shape},\\n\"\n",
    "      f\"\\tmodel=model,\\n\"\n",
    "      f\"\\tmodel_config=model_config,\\n\"\n",
    "      f\"\\ttokenizer=tokenizer\")\n",
    "print(f\"Outputs: clean_logits={clean_logits.shape}, interv_logits={interv_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed FV shape: torch.Size([1, 4096])\n",
      "Top Heads: [(15, 5, 0.0587), (9, 14, 0.0584), (12, 10, 0.0526), (8, 1, 0.0445), (11, 0, 0.0445), (13, 13, 0.019), (8, 0, 0.0184), (14, 9, 0.016), (9, 2, 0.0127), (24, 6, 0.0113)]\n"
     ]
    }
   ],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)\n",
    "print(\"Computed FV shape:\", FV.shape)\n",
    "print(\"Top Heads:\", top_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 16, 97, 256])\n"
     ]
    }
   ],
   "source": [
    "print(mean_activations.shape) # (n_layers, n_heads, n_tokens, head_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 * 256 = 4096\n"
     ]
    }
   ],
   "source": [
    "print(model_config[\"n_heads\"], \"*\", 256, \"=\", model_config[\"n_heads\"] * 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "TraceDict([('transformer.h.0.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c6410>), ('transformer.h.1.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c6800>), ('transformer.h.2.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c6290>), ('transformer.h.3.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c6260>), ('transformer.h.4.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c4370>), ('transformer.h.5.attn.out_proj', <baukit.nethook.Trace object at 0x73baf63c5c30>), ('transformer.h.6.attn.out_proj', <baukit.nethook.Trace object at 0x73baf7772e90>), ('transformer.h.7.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6425270>), ('transformer.h.8.attn.out_proj', <baukit.nethook.Trace object at 0x73baf64255a0>), ('transformer.h.9.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6425bd0>), ('transformer.h.10.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424e80>), ('transformer.h.11.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6425000>), ('transformer.h.12.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6426890>), ('transformer.h.13.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424790>), ('transformer.h.14.attn.out_proj', <baukit.nethook.Trace object at 0x73baf64247f0>), ('transformer.h.15.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424190>), ('transformer.h.16.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6427f40>), ('transformer.h.17.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424b20>), ('transformer.h.18.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6426560>), ('transformer.h.19.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424940>), ('transformer.h.20.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6427ca0>), ('transformer.h.21.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6427a90>), ('transformer.h.22.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6427b80>), ('transformer.h.23.attn.out_proj', <baukit.nethook.Trace object at 0x73baf64276a0>), ('transformer.h.24.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6427370>), ('transformer.h.25.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6424f70>), ('transformer.h.26.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6425900>), ('transformer.h.27.attn.out_proj', <baukit.nethook.Trace object at 0x73baf6425de0>)])\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 55: 53, 56: 54, 57: 55, 58: 56, 59: 57, 60: 58, 61: 59, 62: 60, 63: 61, 64: 62, 65: 63, 66: 64, 67: 65, 68: 66, 69: 67, 70: 68, 71: 69, 72: 70, 73: 71, 74: 72, 75: 73, 76: 74, 77: 75, 78: 76, 79: 77, 80: 78, 81: 79, 82: 80, 83: 81, 84: 82, 85: 83, 86: 84, 87: 85, 88: 86, 89: 87, 90: 88, 91: 89, 92: 90, 93: 91, 94: 92, 95: 93, 96: 94, 97: 95, 98: 96}\n",
      "{'demonstration_3_label_token': (25, 26), 'demonstration_6_label_token': (53, 54)}\n",
      "torch.Size([1, 28, 16, 97, 256])\n",
      "torch.Size([28, 16, 97, 256])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.prompt_utils import get_dummy_token_labels\n",
    "from src.utils.extract_utils import gather_attn_activations\n",
    "\n",
    "def get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer, n_icl_examples = 10, N_TRIALS = 1, shuffle_labels=False, prefixes=None, separators=None, filter_set=None):\n",
    "    def split_activations_by_head(activations, model_config):\n",
    "        new_shape = activations.size()[:-1] + (model_config['n_heads'], model_config['resid_dim']//model_config['n_heads']) # split by head: + (n_attn_heads, hidden_size/n_attn_heads)\n",
    "        activations = activations.view(*new_shape)  # (batch_size, n_tokens, n_heads, head_hidden_dim)\n",
    "        return activations\n",
    "\n",
    "    n_test_examples = 1\n",
    "    if prefixes is not None and separators is not None:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, prefixes=prefixes, separators=separators, model_config=model_config)\n",
    "    else:\n",
    "        dummy_labels = get_dummy_token_labels(n_icl_examples, tokenizer=tokenizer, model_config=model_config)\n",
    "    print(len(dummy_labels))\n",
    "    activation_storage = torch.zeros(N_TRIALS, model_config['n_layers'], model_config['n_heads'], len(dummy_labels), model_config['resid_dim']//model_config['n_heads'])\n",
    "\n",
    "    if filter_set is None:\n",
    "        filter_set = np.arange(len(dataset['valid']))\n",
    "\n",
    "    # If the model already prepends a bos token by default, we don't want to add one\n",
    "    prepend_bos =  False if model_config['prepend_bos'] else True\n",
    "\n",
    "    all_prompt_data = []\n",
    "    for n in range(N_TRIALS):\n",
    "        word_pairs = dataset['train'][np.random.choice(len(dataset['train']),n_icl_examples, replace=False)]\n",
    "        word_pairs_test = dataset['valid'][np.random.choice(filter_set,n_test_examples, replace=False)]\n",
    "        if prefixes is not None and separators is not None:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                    shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "        else:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n",
    "        all_prompt_data.append(prompt_data)\n",
    "        # print(prompt_data)\n",
    "        activations_td,idx_map,idx_avg = gather_attn_activations(prompt_data=prompt_data, \n",
    "                                                            layers = model_config['attn_hook_names'], \n",
    "                                                            dummy_labels=dummy_labels, \n",
    "                                                            model=model, \n",
    "                                                            tokenizer=tokenizer, \n",
    "                                                            model_config=model_config)\n",
    "        print(activations_td)\n",
    "        print(idx_map)\n",
    "        print(idx_avg)\n",
    "        # print(activations_td[layer].input.shape for layer in model_config['attn_hook_names'])\n",
    "        stack_initial = torch.vstack([split_activations_by_head(activations_td[layer].input, model_config) for layer in model_config['attn_hook_names']]).permute(0,2,1,3)\n",
    "        stack_filtered = stack_initial[:,:,list(idx_map.keys())]\n",
    "        for (i,j) in idx_avg.values():\n",
    "            stack_filtered[:,:,idx_map[i]] = stack_initial[:,:,i:j+1].mean(axis=2) # Average activations of multi-token words across all its tokens\n",
    "        \n",
    "        activation_storage[n] = stack_filtered\n",
    "\n",
    "    print(activation_storage.shape)\n",
    "    mean_activations = activation_storage.mean(dim=0) # averaging over trials (not tokens => 97 represents n_tokens)\n",
    "    print(mean_activations.shape)\n",
    "    return mean_activations, all_prompt_data\n",
    "\n",
    "mean_activations, all_prompt_data = get_mean_head_activations_unpacked(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Q: hardware\n",
      "A: software\n",
      "\n",
      "Q: fascism\n",
      "A: democracy\n",
      "\n",
      "Q: incompatible\n",
      "A: compatible\n",
      "\n",
      "Q: illness\n",
      "A: health\n",
      "\n",
      "Q: notice\n",
      "A: ignore\n",
      "\n",
      "Q: lavish\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "from src.utils.prompt_utils import get_token_meta_labels\n",
    "\n",
    "query = all_prompt_data[6]['query_target']['input']\n",
    "token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query, prepend_bos=model_config['prepend_bos'])\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6b\n",
      "ICLDataset({\n",
      "\tfeatures: ['input', 'output'],\n",
      "\tnum_rows: 1678\n",
      "})\n",
      "{\n",
      "  \"input\": [\n",
      "    \"hardware\",\n",
      "    \"fascism\",\n",
      "    \"incompatible\",\n",
      "    \"illness\",\n",
      "    \"notice\"\n",
      "  ],\n",
      "  \"output\": [\n",
      "    \"software\",\n",
      "    \"democracy\",\n",
      "    \"compatible\",\n",
      "    \"health\",\n",
      "    \"ignore\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model_config[\"name_or_path\"])\n",
    "print(dataset[\"train\"])\n",
    "print(json.dumps(dataset[\"train\"][:5], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
